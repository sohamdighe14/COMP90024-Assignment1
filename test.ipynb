{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'cell': 'A1',\n",
       "  'xmin': 149.79255080532937,\n",
       "  'ymin': -33.31644989181766,\n",
       "  'xmax': 150.34255080532938,\n",
       "  'ymax': -32.81644989181766},\n",
       " {'id': 2,\n",
       "  'cell': 'B1',\n",
       "  'xmin': 149.79255080532937,\n",
       "  'ymin': -33.81644989181766,\n",
       "  'xmax': 150.34255080532938,\n",
       "  'ymax': -33.31644989181766},\n",
       " {'id': 3,\n",
       "  'cell': 'C1',\n",
       "  'xmin': 149.79255080532937,\n",
       "  'ymin': -34.31644989181766,\n",
       "  'xmax': 150.34255080532938,\n",
       "  'ymax': -33.81644989181766},\n",
       " {'id': 4,\n",
       "  'cell': 'D1',\n",
       "  'xmin': 149.79255080532937,\n",
       "  'ymin': -34.81644989181766,\n",
       "  'xmax': 150.34255080532938,\n",
       "  'ymax': -34.31644989181766},\n",
       " {'id': 5,\n",
       "  'cell': 'A2',\n",
       "  'xmin': 150.34255080532938,\n",
       "  'ymin': -33.31644989181766,\n",
       "  'xmax': 150.8925508053294,\n",
       "  'ymax': -32.81644989181766},\n",
       " {'id': 6,\n",
       "  'cell': 'B2',\n",
       "  'xmin': 150.34255080532938,\n",
       "  'ymin': -33.81644989181766,\n",
       "  'xmax': 150.8925508053294,\n",
       "  'ymax': -33.31644989181766},\n",
       " {'id': 7,\n",
       "  'cell': 'C2',\n",
       "  'xmin': 150.34255080532938,\n",
       "  'ymin': -34.31644989181766,\n",
       "  'xmax': 150.8925508053294,\n",
       "  'ymax': -33.81644989181766},\n",
       " {'id': 8,\n",
       "  'cell': 'D2',\n",
       "  'xmin': 150.34255080532938,\n",
       "  'ymin': -34.81644989181766,\n",
       "  'xmax': 150.8925508053294,\n",
       "  'ymax': -34.31644989181766},\n",
       " {'id': 9,\n",
       "  'cell': 'A3',\n",
       "  'xmin': 150.89255080532936,\n",
       "  'ymin': -33.31644989181766,\n",
       "  'xmax': 151.44255080532938,\n",
       "  'ymax': -32.81644989181766},\n",
       " {'id': 10,\n",
       "  'cell': 'B3',\n",
       "  'xmin': 150.89255080532936,\n",
       "  'ymin': -33.81644989181766,\n",
       "  'xmax': 151.44255080532938,\n",
       "  'ymax': -33.31644989181766},\n",
       " {'id': 11,\n",
       "  'cell': 'C3',\n",
       "  'xmin': 150.89255080532936,\n",
       "  'ymin': -34.31644989181766,\n",
       "  'xmax': 151.44255080532938,\n",
       "  'ymax': -33.81644989181766},\n",
       " {'id': 12,\n",
       "  'cell': 'D3',\n",
       "  'xmin': 150.89255080532936,\n",
       "  'ymin': -34.81644989181766,\n",
       "  'xmax': 151.44255080532938,\n",
       "  'ymax': -34.31644989181766},\n",
       " {'id': 13,\n",
       "  'cell': 'A4',\n",
       "  'xmin': 151.44255080532938,\n",
       "  'ymin': -33.31644989181766,\n",
       "  'xmax': 151.9925508053294,\n",
       "  'ymax': -32.81644989181766},\n",
       " {'id': 14,\n",
       "  'cell': 'B4',\n",
       "  'xmin': 151.44255080532938,\n",
       "  'ymin': -33.81644989181766,\n",
       "  'xmax': 151.9925508053294,\n",
       "  'ymax': -33.31644989181766},\n",
       " {'id': 15,\n",
       "  'cell': 'C4',\n",
       "  'xmin': 151.44255080532938,\n",
       "  'ymin': -34.31644989181766,\n",
       "  'xmax': 151.9925508053294,\n",
       "  'ymax': -33.81644989181766},\n",
       " {'id': 16,\n",
       "  'cell': 'D4',\n",
       "  'xmin': 151.44255080532938,\n",
       "  'ymin': -34.81644989181766,\n",
       "  'xmax': 151.9925508053294,\n",
       "  'ymax': -34.31644989181766}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "def SydGrid(file_name):\n",
    "    '''\n",
    "    \"geo\":{\"type\":\"Point\",\"coordinates\":[-33.86751,151.20797]}\n",
    "    all 8 acesss and read same time\n",
    "    datastructures to optimize performance\n",
    "    showq -q snowy| less\n",
    "    master has rank 0 always\n",
    "    sbatch <script_name> \n",
    "    squeue -u <username>\n",
    "    module avail mpip4py//list all libs\n",
    "    tell module load <lib> in slurm script\n",
    "    scancel <job_id>\n",
    "    '''\n",
    "    syd_grid = []\n",
    "    with open(file_name) as f:\n",
    "        data = json.load(f)\n",
    "        for val in data['features']:\n",
    "            grid_data = {}\n",
    "            properties = val['properties']\n",
    "            grid_data['id'] = properties['id']\n",
    "            num = int((grid_data['id'])%4)\n",
    "            if num == 0:\n",
    "                num=4\n",
    "            grid_data['cell'] = chr(int(num+64))+str(int((grid_data['id']-1)/4)+1)\n",
    "            geometry = val['geometry'] \n",
    "            grid_data['xmin'] = geometry['coordinates'][0][0][0]\n",
    "            grid_data['ymin'] = geometry['coordinates'][0][2][1]\n",
    "            grid_data['xmax'] = geometry['coordinates'][0][1][0]\n",
    "            grid_data['ymax'] = geometry['coordinates'][0][0][1]\n",
    "            syd_grid.append(grid_data)\n",
    "    return syd_grid\n",
    "grid = SydGrid('./sydGrid.json')\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fr': 'French', 'en': 'English', 'ar': 'Arabic', 'ja': 'Japanese', 'es': 'Spanish', 'de': 'German', 'it': 'Italian', 'in': 'Indonesian', 'pt': 'Portuguese', 'ko': 'Korean', 'tr': 'Turkish', 'ru': 'Russian', 'nl': 'Dutch', 'fil': 'Filipino', 'msa': 'Malay', 'zh': 'Chinese', 'hi': 'Hindi', 'no': 'Norwegian', 'sv': 'Swedish', 'fi': 'Finnish', 'da': 'Danish', 'pl': 'Polish', 'hu': 'Hungarian', 'fa': 'Persian', 'he': 'Hebrew', 'ur': 'Urdu', 'th': 'Thai', 'uk': 'Ukrainian', 'ca': 'Catalan', 'ga': 'Irish', 'el': 'Greek', 'eu': 'Basque', 'cs': 'Czech', 'gl': 'Galician', 'ro': 'Romanian', 'hr': 'Croatian', 'vi': 'Vietnamese', 'bn': 'Bangla', 'bg': 'Bulgarian', 'sr': 'Serbian', 'sk': 'Slovak', 'gu': 'Gujarati', 'mr': 'Marathi', 'ta': 'Tamil', 'kn': 'Kannada'}\n"
     ]
    }
   ],
   "source": [
    "def LangCodes(file_name):\n",
    "    '''\n",
    "    output is kv pair with lang_code:lang\n",
    "    '''\n",
    "    lang_codes = dict()\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "            data = json.load(f) \n",
    "            for val in data:\n",
    "                lang_codes[val['code']] = val['name']\n",
    "            return lang_codes\n",
    "lc = LangCodes('./language_codes.json')\n",
    "print((lc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(tweet):\n",
    "    tweet_data = {}\n",
    "    if type(tweet['geo'])!= type(None):\n",
    "        language = tweet['doc']['metadata']['iso_language_code']\n",
    "        x = tweet['doc']['geo']['coordinates'][1]\n",
    "        y = tweet['doc']['geo']['coordinates'][0]\n",
    "        #checking if the point lies in grid\n",
    "        if 151.992551 < x < 149.792551 and -32.81645 < y < -34.81645:\n",
    "            tweet_data['language'] = language\n",
    "            tweet_data['grid_cell'] = GetGridCell(x,y)\n",
    "    return tweet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_language_count(dict_lang, lang_codes):\n",
    "    \"\"\"Print the results from language code counting.\n",
    "    Keyword arguments:\n",
    "    counter_language -- counter with language counts\n",
    "    supported_languages -- dict mapping lancode to name\n",
    "    \"\"\"\n",
    "    for item in dict_lang:\n",
    "        print(\"Cell :\",item)\n",
    "        print('# tweets', sum(dict_lang[item].values()))\n",
    "        print('# languages :', len(dict_lang[item]))\n",
    "        counter_language = dict_lang[item]\n",
    "        count = 1\n",
    "        print(\"Most common languages in dataset:\", flush=True)\n",
    "        for language in counter_language.most_common(10):\n",
    "            #if there is a bug its here\n",
    "            print(str(count) + \". \" + lang_codes[language[0]] + \" (\" + language[0] + \")\" + \", \" + str(language[1]), flush=True)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_chunks(file_name, chunk_size, total_size):\n",
    "    with open(file_name,'rb') as f:\n",
    "        #position pointer for file \n",
    "        chunk_end = f.tell()\n",
    "        while True:\n",
    "            # we update this for every \n",
    "            chunk_start = chunk_end\n",
    "            # define ending of the chunk\n",
    "            f.seek(f.tell()+chunk_size)\n",
    "            # read everyline \n",
    "            f.readline()\n",
    "            #checking if the line is inside the chunk \n",
    "            chunk_end = f.tell()\n",
    "            #getting the size required\n",
    "            if chunk_end > total_size:\n",
    "                chunk_end = total_size\n",
    "            yield chunk_start, chunk_end-chunk_start\n",
    "            if chunk_end == total_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_batches(file_path, chunk_start, chunk_size, batch_size):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        batch_end = chunk_start\n",
    "\n",
    "        while True:\n",
    "            batch_start = batch_end\n",
    "            # go to current position + batch_size\n",
    "            f.seek(batch_start + batch_size)\n",
    "            # read the lines as a whole\n",
    "            f.readline()\n",
    "            batch_end = f.tell()\n",
    "            if batch_end > chunk_start + chunk_size:\n",
    "                batch_end = chunk_start + chunk_size\n",
    "            yield batch_start, batch_end - batch_start\n",
    "            # reaching the end of chunk\n",
    "            if batch_end == chunk_start + chunk_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetGridCell(x,y):\n",
    "    grid = SydGrid('./sydGrid.json')\n",
    "    for item in grid:\n",
    "        if item['xmin']<x<=item['xmax'] and item['ymin']<y<=item['ymax']:\n",
    "            return item['cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class DataProcessor():\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size #BATCH_SIZE in repo\n",
    "        self.lang_counter = dict(Counter())\n",
    "    \n",
    "    def get_results(self): #retrive_results in repo\n",
    "        result = {\"language\": self.lang_counter}\n",
    "        return result\n",
    "\n",
    "    def process_tweet(self, tweet):\n",
    "        \"\"\"Process tweet and perform counting operations\n",
    "        Keyword arguments:\n",
    "        tweet -- tweet in JSON format\n",
    "        \"\"\"\n",
    "        # Extract language\n",
    "        data = GetData(tweet)\n",
    "        language = data['language']\n",
    "        cell = data['grid_cell']\n",
    "        self.lang_counter[cell][language] += 1\n",
    "        \n",
    "\n",
    "    def process_wrapper(self, path_to_dataset, chunk_start, chunk_size):\n",
    "        \"\"\"Main method executed by worker process to split chunk into smaller\n",
    "        batches and process batches sequentially\n",
    "        Keyword arguments:\n",
    "        path_to_dataset -- Path to dataset to be split up\n",
    "        chunk_start -- Byte offset of chunk from beginning of file\n",
    "        chunk_size -- Size of chunk in bytes\n",
    "        \"\"\"\n",
    "        with open(path_to_dataset, 'rb') as f:\n",
    "            batches = []\n",
    "\n",
    "            # Split up chunk into batches of size BATCH_SIZE\n",
    "            for read_start, read_size in break_batches(path_to_dataset, chunk_start, chunk_size, self.batch_size):\n",
    "                batches.append({\"batchStart\": read_start, \"batchSize\": read_size})\n",
    "\n",
    "            # Process batches sequentially\n",
    "            for batch in batches:\n",
    "\n",
    "                # Move to start position of batch\n",
    "                f.seek(batch['batchStart'])\n",
    "\n",
    "                if batch['batchSize'] > 0:\n",
    "                    # Read in next batch in bytes as given per batchSize and\n",
    "                    # split lines\n",
    "                    content = f.read(batch['batchSize']).splitlines()\n",
    "\n",
    "                    for line in content:\n",
    "                        # Decode each line as utf-8 string\n",
    "                        line = line.decode('utf-8')  # Convert to utf-8\n",
    "                        if line[-1] == \",\":  # if line has comma\n",
    "                            line = line[:-1]  # removing trailing comma\n",
    "                        try:\n",
    "                            # Load tweet in JSON format\n",
    "                            tweet = json.loads(line)\n",
    "                            self.process_tweet(tweet)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(\"Error reading row from JSON file - ignoring\")\n",
    "                            print(line)\n",
    "                else:\n",
    "                    print(\"batchsize with size 0 detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Get start time for the program\n",
    "STARTTIME = dt.now()\n",
    "ENDTIME = None\n",
    "\n",
    "# Start doing MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "dataset = 'smallTwitter.json'\n",
    "langcodes = 'language_codes.json'\n",
    "sydGrid = \"sydGrid.json\"\n",
    "\n",
    "\"\"\"\n",
    "argParser = argparse.ArgumentParser()\n",
    "argParser.add_argument('--dataset', type = str, default = 'bigTwitter.json')\n",
    "argParser.add_argument('--langcodes', type = str, default = 'language_codes.json')\n",
    "args = argParser.parse_args()\n",
    "\"\"\"\n",
    "\n",
    "dataSetPath = \"./\" + dataset\n",
    "codesPath = \"./\" + langcodes\n",
    "gridPath = \"./\" + sydGrid\n",
    "\n",
    "lc = LangCodes(codesPath)\n",
    "grid = SydGrid(gridPath)\n",
    "\n",
    "def main():\n",
    "    dataProcessor = DataProcessor()\n",
    "    if RANK == 0:\n",
    "        dataTotSize = os.path.getsize(dataSetPath)\n",
    "        sizePerProcess = dataTotSize / SIZE\n",
    "        \n",
    "        chunks = []\n",
    "        for chunkStart, chunkSize in break_chunks(dataSetPath,\n",
    "                                              int(sizePerProcess),\n",
    "                                              dataTotSize):\n",
    "            chunks.append({\"chunkStart\": chunkStart, \"chunkSize\": chunkSize})\n",
    "    else:\n",
    "        chunks = None\n",
    "        \n",
    "    COMM.Barrier()\n",
    "    \n",
    "    ## Still waiting on helper functions to be finished and defined before we can fill in blanks\n",
    "    \n",
    "    processorChunks = COMM.scatter(chunks, root=0)\n",
    "    \n",
    "    if RANK != 0:\n",
    "        ENDTIME = dt.now()\n",
    "        \n",
    "    COMM.Barrier()\n",
    "    \n",
    "    ## Still waiting on helper functions to be finished and defined before we can fill in blanks\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l 2\n",
      "h 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "f =dict(Counter())\n",
    "a='hi'\n",
    "f['hi'] = Counter('hello')\n",
    "f['bi'] = Counter('bello')\n",
    "cntr = f[a]\n",
    "for language in cntr.most_common(2):\n",
    "    print(language[0],language[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f30f52230e98a66701697f317046da00ce2cde01feb9a5533ae28cf5631ecd9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
